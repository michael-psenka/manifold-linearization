{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 82,
=======
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_mimicry as mmc\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch_mimicry.nets.dcgan import dcgan_base\n",
    "from torch_mimicry.modules.layers import SNLinear\n",
    "from torch_mimicry.nets import sngan\n",
    "from torch_mimicry.nets.sngan.sngan_128 import SNGANDiscriminator128\n",
    "from torch_mimicry.nets.sngan.sngan_48 import SNGANDiscriminator48\n",
    "from torch_mimicry.nets.sngan.sngan_32 import SNGANDiscriminator32\n",
    "from torch_mimicry.training import scheduler, logger, metric_log\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms.functional as FF\n",
    "import torchvision.utils as vutils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x)\n",
    "\n",
    "\n",
    "class customSNGANDiscriminator128(SNGANDiscriminator128):\n",
    "\n",
    "    def __init__(self, nz=128, ndf=1024, **kwargs):\n",
    "        super(customSNGANDiscriminator128, self).__init__(ndf, **kwargs)\n",
    "        self.nz = nz\n",
    "        self.l7 = nn.Sequential(SNLinear(self.ndf, nz), Norm())\n",
    "\n",
    "\n",
    "class customSNGANDiscriminator48(SNGANDiscriminator48):\n",
    "\n",
    "    def __init__(self, nz=128, ndf=1024, **kwargs):\n",
    "        super(customSNGANDiscriminator48, self).__init__(ndf, **kwargs)\n",
    "        self.nz = nz\n",
    "        self.l5 = nn.Sequential(SNLinear(self.ndf, nz), Norm())\n",
    "\n",
    "\n",
    "class customSNGANDiscriminator32(SNGANDiscriminator32):\n",
    "\n",
    "    def __init__(self, nz=128, ndf=128, **kwargs):\n",
    "        super(customSNGANDiscriminator32, self).__init__(ndf, **kwargs)\n",
    "        self.nz = nz\n",
    "        self.l5 = nn.Sequential(SNLinear(self.ndf, nz), Norm())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "class GeneratorMNIST(dcgan_base.DCGANBaseGenerator):\n",
    "    r\"\"\"\n",
    "    ResNet backbone generator for ResNet DCGAN.\n",
    "    Attributes:\n",
    "        nz (int): Noise dimension for upsampling.\n",
    "        ngf (int): Variable controlling generator feature map sizes.\n",
    "        bottom_width (int): Starting width for upsampling generator output to an image.\n",
    "        loss_type (str): Name of loss to use for GAN loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, nz=128, ngf=64, bottom_width=4, **kwargs):\n",
    "        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n",
    "\n",
    "        # self.main = nn.Sequential(\n",
    "        #     nn.Linear(nz, ngf * 16),\n",
    "        #     nn.BatchNorm1d(ngf * 16),\n",
    "        #     nn.ReLU(True),\n",
    "        #     nn.Linear(ngf * 16, 7*7*128),\n",
    "        #     nn.BatchNorm1d(7*7*128),\n",
    "        #     nn.ReLU(True),\n",
    "        #     nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "        #     nn.BatchNorm2d(ngf * 2),\n",
    "        #     nn.ReLU(True),\n",
    "        #     nn.ConvTranspose2d(ngf * 2, 1, 4, 2, 1, bias=False),\n",
    "        #     nn.Tanh()\n",
    "        # )\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(nz, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 7*7*128),\n",
    "            nn.BatchNorm1d(7*7*128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"\n",
    "        Feedforwards a batch of noise vectors into a batch of fake images.\n",
    "        Args:\n",
    "            x (Tensor): A batch of noise vectors of shape (N, nz).\n",
    "        Returns:\n",
    "            Tensor: A batch of fake images of shape (N, C, H, W).\n",
    "        \"\"\"\n",
    "        # h = h.view(x.shape[0], -1, 1, 1)\n",
    "        return self.main(x.view(x.shape[0], -1, 1, 1))\n",
    "\n",
    "\n",
    "class DiscriminatorMNIST(dcgan_base.DCGANBaseDiscriminator):\n",
    "    r\"\"\"\n",
    "    ResNet backbone discriminator for ResNet DCGAN.\n",
    "    Attributes:\n",
    "        ndf (int): Variable controlling discriminator feature map sizes.\n",
    "        loss_type (str): Name of loss to use for GAN loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, nz=128, ndf=64, **kwargs):\n",
    "        super().__init__(ndf=ndf, **kwargs)\n",
    "\n",
    "        self.nz = nz\n",
    "        # self.main = nn.Sequential(\n",
    "        #     nn.Conv2d(1, ndf, 4, 2, 1, bias=False),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "        #     nn.BatchNorm2d(ndf * 2),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "        #     nn.BatchNorm2d(ndf * 4),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     nn.Conv2d(ndf * 4, nz, 4, 1, 0, bias=False),\n",
    "        #     nn.Flatten()  # new\n",
    "        #     # nn.LeakyReLU(0.2, inplace=True), #New\n",
    "        #     # nn.Linear(ndf, ndf, bias=False)\n",
    "        #     # nn.Sigmoid()\n",
    "        # )\n",
    "        self.main = nn.Sequential(# 28 -> 14\n",
    "            nn.Conv2d(1, 64, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128*7*7, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n",
    "        Args:\n",
    "            x (Tensor): A batch of images of shape (N, C, H, W).\n",
    "        Returns:\n",
    "            Tensor: A batch of GAN logits of shape (N, 1).\n",
    "        \"\"\"\n",
    "        return F.normalize(self.main(x))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "def weights_init_mnist_model(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "class MCRGANloss(nn.Module):\n",
    "\n",
    "    def __init__(self, gam1=1., gam2=1., gam3=1., eps=0.5, numclasses=1000, mode=1, rho=None):\n",
    "        super(MCRGANloss, self).__init__()\n",
    "\n",
    "        self.num_class = numclasses\n",
    "        self.train_mode = mode\n",
    "        self.faster_logdet = False\n",
    "        self.gam1 = gam1\n",
    "        self.gam2 = gam2\n",
    "        self.gam3 = gam3\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, Z, Z_bar, real_label, ith_inner_loop, num_inner_loop):\n",
    "\n",
    "        # t = time.time()\n",
    "        # errD, empi = self.old_version(Z, Z_bar, real_label, ith_inner_loop, num_inner_loop)\n",
    "        errD, empi = self.fast_version(Z, Z_bar, real_label, ith_inner_loop, num_inner_loop)\n",
    "        # print(\"faster version time: \", time.time() - t)\n",
    "        # print(\"faster errD\", errD)\n",
    "\n",
    "        return errD, empi\n",
    "\n",
    "    def old_version(self, Z, Z_bar, real_label, ith_inner_loop, num_inner_loop):\n",
    "\n",
    "        \"\"\" original version, need to calculate 52 times log-det\"\"\"\n",
    "        if self.train_mode == 2:\n",
    "            loss_z, _ = self.deltaR(Z, real_label, self.num_class)\n",
    "            assert num_inner_loop >= 2\n",
    "            if (ith_inner_loop + 1) % num_inner_loop != 0:\n",
    "                return loss_z, None\n",
    "\n",
    "            loss_h, _ = self.deltaR(Z_bar, real_label, self.num_class)\n",
    "            errD = self.gam1 * loss_z + self.gam2 * loss_h\n",
    "            empi = [loss_z, loss_h]\n",
    "            term3 = 0.\n",
    "\n",
    "            for i in range(self.num_class):\n",
    "                new_Z = torch.cat((Z[real_label == i], Z_bar[real_label == i]), 0)\n",
    "                new_label = torch.cat(\n",
    "                    (torch.zeros_like(real_label[real_label == i]),\n",
    "                     torch.ones_like(real_label[real_label == i]))\n",
    "                )\n",
    "                loss, _ = self.deltaR(new_Z, new_label, 2)\n",
    "                term3 += loss\n",
    "            empi = empi + [term3]\n",
    "            errD += self.gam3 * term3\n",
    "\n",
    "        elif self.train_mode == 1:\n",
    "\n",
    "            loss_z, _ = self.deltaR(Z, real_label, self.num_class)\n",
    "            loss_h, _ = self.deltaR(Z_bar, real_label, self.num_class)\n",
    "            errD = self.gam1 * loss_z + self.gam2 * loss_h\n",
    "            empi = [loss_z, loss_h]\n",
    "            term3 = 0.\n",
    "\n",
    "            for i in range(self.num_class):\n",
    "                new_Z = torch.cat((Z[real_label == i], Z_bar[real_label == i]), 0)\n",
    "                new_label = torch.cat(\n",
    "                    (torch.zeros_like(real_label[real_label == i]),\n",
    "                     torch.ones_like(real_label[real_label == i]))\n",
    "                )\n",
    "                loss, _ = self.deltaR(new_Z, new_label, 2)\n",
    "                term3 += loss\n",
    "            empi = empi + [term3]\n",
    "            errD += self.gam3 * term3\n",
    "        elif self.train_mode == 0:\n",
    "            new_Z = torch.cat((Z, Z_bar), 0)\n",
    "            new_label = torch.cat((torch.zeros_like(real_label), torch.ones_like(real_label)))\n",
    "            errD, em = self.deltaR(new_Z, new_label, 2)\n",
    "            empi = (em[0], em[1])\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return errD, empi\n",
    "\n",
    "    def fast_version(self, Z, Z_bar, real_label, ith_inner_loop, num_inner_loop):\n",
    "\n",
    "        \"\"\" decrease the times of calculate log-det  from 52 to 32\"\"\"\n",
    "\n",
    "        if self.train_mode == 2:\n",
    "            z_total, (z_discrimn_item, z_compress_item, z_compress_losses, z_scalars) = self.deltaR(Z, real_label,\n",
    "                                                                                                    self.num_class)\n",
    "            assert num_inner_loop >= 2\n",
    "            if (ith_inner_loop + 1) % num_inner_loop != 0:\n",
    "                # print(f\"{ith_inner_loop + 1}/{num_inner_loop}\")\n",
    "                # print(\"calculate delta R(z)\")\n",
    "                return z_total, None\n",
    "\n",
    "            zbar_total, (zbar_discrimn_item, zbar_compress_item, zbar_compress_losses, zbar_scalars) = self.deltaR(\n",
    "                Z_bar, real_label, self.num_class)\n",
    "            empi = [z_total, zbar_total]\n",
    "\n",
    "            itemRzjzjbar = 0.\n",
    "            for j in range(self.num_class):\n",
    "                new_z = torch.cat((Z[real_label == j], Z_bar[real_label == j]), 0)\n",
    "                R_zjzjbar = self.compute_discrimn_loss(new_z.T)\n",
    "                itemRzjzjbar += R_zjzjbar\n",
    "\n",
    "            errD_ = self.gam1 * (z_discrimn_item - z_compress_item) + \\\n",
    "                    self.gam2 * (zbar_discrimn_item - zbar_compress_item) + \\\n",
    "                    self.gam3 * (itemRzjzjbar - 0.25 * sum(z_compress_losses) - 0.25 * sum(zbar_compress_losses))\n",
    "            errD = -errD_\n",
    "\n",
    "            empi = empi + [-itemRzjzjbar + 0.25 * sum(z_compress_losses) + 0.25 * sum(zbar_compress_losses)]\n",
    "            # print(\"calculate multi\")\n",
    "\n",
    "        elif self.train_mode == 1:\n",
    "            z_total, (z_discrimn_item, z_compress_item, z_compress_losses, z_scalars) = self.deltaR(Z, real_label, self.num_class)\n",
    "            zbar_total, (zbar_discrimn_item, zbar_compress_item, zbar_compress_losses, zbar_scalars) = self.deltaR(Z_bar, real_label, self.num_class)\n",
    "            empi = [z_total, zbar_total]\n",
    "\n",
    "            itemRzjzjbar = 0.\n",
    "            for j in range(self.num_class):\n",
    "                new_z = torch.cat((Z[real_label == j], Z_bar[real_label == j]), 0)\n",
    "                R_zjzjbar = self.compute_discrimn_loss(new_z.T)\n",
    "                itemRzjzjbar += R_zjzjbar\n",
    "\n",
    "            errD_ = self.gam1 * (z_discrimn_item - z_compress_item) + \\\n",
    "                    self.gam2 * (zbar_discrimn_item - zbar_compress_item) + \\\n",
    "                    self.gam3 * (itemRzjzjbar - 0.25 * sum(z_compress_losses) - 0.25 * sum(zbar_compress_losses))\n",
    "            errD = -errD_\n",
    "\n",
    "            empi = empi + [-itemRzjzjbar + 0.25 * sum(z_compress_losses) + 0.25 * sum(zbar_compress_losses)]\n",
    "\n",
    "        elif self.train_mode == 0:\n",
    "            new_Z = torch.cat((Z, Z_bar), 0)\n",
    "            new_label = torch.cat((torch.zeros_like(real_label), torch.ones_like(real_label)))\n",
    "            errD, extra = self.deltaR(new_Z, new_label, 2)\n",
    "            empi = (extra[0], extra[1])\n",
    "\n",
    "        elif self.train_mode == 10:\n",
    "            errD, empi = self.double_loop(Z, Z_bar, real_label, ith_inner_loop, num_inner_loop)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return errD, empi\n",
    "\n",
    "    def logdet(self, X):\n",
    "\n",
    "        if self.faster_logdet:\n",
    "            return 2 * torch.sum(torch.log(torch.diag(torch.linalg.cholesky(X, upper=True))))\n",
    "        else:\n",
    "            return torch.logdet(X)\n",
    "\n",
    "    def compute_discrimn_loss(self, Z):\n",
    "        \"\"\"Theoretical Discriminative Loss.\"\"\"\n",
    "        d, n = Z.shape\n",
    "        I = torch.eye(d).to(Z.device)\n",
    "        scalar = d / (n * self.eps)\n",
    "        logdet = self.logdet(I + scalar * Z @ Z.T)\n",
    "        return logdet / 2.\n",
    "\n",
    "    def compute_compress_loss(self, Z, Pi):\n",
    "        \"\"\"Theoretical Compressive Loss.\"\"\"\n",
    "        d, n = Z.shape\n",
    "        I = torch.eye(d).to(Z.device)\n",
    "        compress_loss = []\n",
    "        scalars = []\n",
    "        for j in range(Pi.shape[1]):\n",
    "            Z_ = Z[:, Pi[:, j] == 1]\n",
    "            trPi = Pi[:, j].sum() + 1e-8\n",
    "            scalar = d / (trPi * self.eps)\n",
    "            log_det = 1. if Pi[:, j].sum() == 0 else self.logdet(I + scalar * Z_ @ Z_.T)\n",
    "            compress_loss.append(log_det)\n",
    "            scalars.append(trPi / (2 * n))\n",
    "        return compress_loss, scalars\n",
    "\n",
    "    def deltaR(self, Z, Y, num_classes):\n",
    "\n",
    "        Pi = F.one_hot(Y, num_classes).to(Z.device)\n",
    "        discrimn_loss = self.compute_discrimn_loss(Z.T)\n",
    "        compress_loss, scalars = self.compute_compress_loss(Z.T, Pi)\n",
    "\n",
    "        compress_term = 0.\n",
    "        for z, s in zip(compress_loss, scalars):\n",
    "            compress_term += s * z\n",
    "        total_loss = discrimn_loss - compress_term\n",
    "\n",
    "        return -total_loss, (discrimn_loss, compress_term, compress_loss, scalars)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "def infiniteloop(dataloader):\n",
    "    while True:\n",
    "        for x, y in iter(dataloader):\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "class MUltiGPUTrainer(mmc.training.Trainer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 netD,\n",
    "                 netG,\n",
    "                 optD,\n",
    "                 optG,\n",
    "                 dataloader,\n",
    "                 num_steps,\n",
    "                 log_dir='./logs',\n",
    "                 n_dis=1,\n",
    "                 lr_decay=None,\n",
    "                 device=None,\n",
    "                 netG_ckpt_file=None,\n",
    "                 netD_ckpt_file=None,\n",
    "                 print_steps=1,\n",
    "                 vis_steps=500,\n",
    "                 log_steps=50,\n",
    "                 save_steps=5000,\n",
    "                 flush_secs=30,\n",
    "                 amp=False):\n",
    "\n",
    "        # Input values checks\n",
    "        ints_to_check = {\n",
    "            'num_steps': num_steps,\n",
    "            'n_dis': n_dis,\n",
    "            'print_steps': print_steps,\n",
    "            'vis_steps': vis_steps,\n",
    "            'log_steps': log_steps,\n",
    "            'save_steps': save_steps,\n",
    "            'flush_secs': flush_secs\n",
    "        }\n",
    "        for name, var in ints_to_check.items():\n",
    "            if var < 1:\n",
    "                raise ValueError('{} must be at least 1 but got {}.'.format(\n",
    "                    name, var))\n",
    "\n",
    "        self.netD = netD\n",
    "        self.netG = netG\n",
    "        self.optD = optD\n",
    "        self.optG = optG\n",
    "        self.n_dis = n_dis\n",
    "        self.lr_decay = lr_decay\n",
    "        self.dataloader = dataloader\n",
    "        self.num_steps = num_steps\n",
    "        self.device = device\n",
    "        self.log_dir = log_dir\n",
    "        self.netG_ckpt_file = netG_ckpt_file\n",
    "        self.netD_ckpt_file = netD_ckpt_file\n",
    "        self.print_steps = print_steps\n",
    "        self.vis_steps = vis_steps\n",
    "        self.log_steps = log_steps\n",
    "        self.save_steps = save_steps\n",
    "        self.amp = amp\n",
    "        self.parallel = isinstance(self.netG, nn.DataParallel)\n",
    "\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        # Training helper objects\n",
    "        self.logger = logger.Logger(log_dir=self.log_dir,\n",
    "                                    num_steps=self.num_steps,\n",
    "                                    dataset_size=len(self.dataloader),\n",
    "                                    flush_secs=flush_secs,\n",
    "                                    device=self.device)\n",
    "\n",
    "        self.scheduler = scheduler.LRScheduler(lr_decay=self.lr_decay,\n",
    "                                               optD=self.optD,\n",
    "                                               optG=self.optG,\n",
    "                                               num_steps=self.num_steps)\n",
    "\n",
    "        # Obtain custom or latest checkpoint files\n",
    "        if self.netG_ckpt_file:\n",
    "            self.netG_ckpt_dir = os.path.dirname(netG_ckpt_file)\n",
    "            self.netG_ckpt_file = netG_ckpt_file\n",
    "        else:\n",
    "            self.netG_ckpt_dir = os.path.join(self.log_dir, 'checkpoints',\n",
    "                                              'netG')\n",
    "            self.netG_ckpt_file = self._get_latest_checkpoint(\n",
    "                self.netG_ckpt_dir)  # can be None\n",
    "\n",
    "        if self.netD_ckpt_file:\n",
    "            self.netD_ckpt_dir = os.path.dirname(netD_ckpt_file)\n",
    "            self.netD_ckpt_file = netD_ckpt_file\n",
    "        else:\n",
    "            self.netD_ckpt_dir = os.path.join(self.log_dir, 'checkpoints',\n",
    "                                              'netD')\n",
    "            self.netD_ckpt_file = self._get_latest_checkpoint(\n",
    "                self.netD_ckpt_dir)\n",
    "\n",
    "        # Log hyperparameters for experiments\n",
    "        self.params = {\n",
    "            'log_dir': self.log_dir,\n",
    "            'num_steps': self.num_steps,\n",
    "            'batch_size': self.dataloader.batch_size,\n",
    "            'n_dis': self.n_dis,\n",
    "            'lr_decay': self.lr_decay,\n",
    "            'optD': optD.__repr__(),\n",
    "            'optG': optG.__repr__(),\n",
    "            'save_steps': self.save_steps,\n",
    "        }\n",
    "        self._log_params(self.params)\n",
    "\n",
    "        # Device for hosting model and data\n",
    "        if not self.device:\n",
    "            self.device = torch.device(\n",
    "                'cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _save_model_checkpoints(self, global_step):\n",
    "        \"\"\"\n",
    "        Saves both discriminator and generator checkpoints.\n",
    "        \"\"\"\n",
    "        if not self.parallel:\n",
    "            self.netG.save_checkpoint(directory=self.netG_ckpt_dir,\n",
    "                                      global_step=global_step,\n",
    "                                      optimizer=self.optG)\n",
    "\n",
    "            self.netD.save_checkpoint(directory=self.netD_ckpt_dir,\n",
    "                                      global_step=global_step,\n",
    "                                      optimizer=self.optD)\n",
    "        else:\n",
    "            self.netG.module.save_checkpoint(directory=self.netG_ckpt_dir,\n",
    "                                             global_step=global_step,\n",
    "                                             optimizer=self.optG)\n",
    "\n",
    "            self.netD.module.save_checkpoint(directory=self.netD_ckpt_dir,\n",
    "                                             global_step=global_step,\n",
    "                                             optimizer=self.optD)\n",
    "\n",
    "    def _restore_models_and_step(self):\n",
    "        \"\"\"\n",
    "        Restores model and optimizer checkpoints and ensures global step is in sync.\n",
    "        \"\"\"\n",
    "        global_step_D = global_step_G = 0\n",
    "\n",
    "        if self.netD_ckpt_file and os.path.exists(self.netD_ckpt_file):\n",
    "            print(\"INFO: Restoring checkpoint for D...\")\n",
    "            global_step_D = self.netD.module.restore_checkpoint(\n",
    "                ckpt_file=self.netD_ckpt_file, optimizer=self.optD)\n",
    "\n",
    "        if self.netG_ckpt_file and os.path.exists(self.netG_ckpt_file):\n",
    "            print(\"INFO: Restoring checkpoint for G...\")\n",
    "            global_step_G = self.netG.module.restore_checkpoint(\n",
    "                ckpt_file=self.netG_ckpt_file, optimizer=self.optG)\n",
    "\n",
    "        if global_step_G != global_step_D:\n",
    "            raise ValueError('G and D Networks are out of sync.')\n",
    "        else:\n",
    "            global_step = global_step_G  # Restores global step\n",
    "\n",
    "        return global_step\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Runs the training pipeline with all given parameters in Trainer.\n",
    "        \"\"\"\n",
    "        # Restore models\n",
    "        global_step = self._restore_models_and_step()\n",
    "        print(\"INFO: Starting training from global step {}...\".format(\n",
    "            global_step))\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Iterate through data\n",
    "            iter_dataloader = iter(self.dataloader)\n",
    "            while global_step < self.num_steps:\n",
    "                log_data = metric_log.MetricLog()  # log data for tensorboard\n",
    "\n",
    "                # -------------------------\n",
    "                #   One Training Step\n",
    "                # -------------------------\n",
    "                # Update n_dis times for D\n",
    "                for i in range(self.n_dis):\n",
    "                    iter_dataloader, real_batch = self._fetch_data(\n",
    "                        iter_dataloader=iter_dataloader)\n",
    "\n",
    "                    # ------------------------\n",
    "                    #   Update D Network\n",
    "                    # -----------------------\n",
    "\n",
    "                    self.netD.module.zero_grad()\n",
    "                    real_images, real_labels = real_batch\n",
    "                    batch_size = real_images.shape[0]  # Match batch sizes for last iter\n",
    "\n",
    "                    # Produce logits for real images\n",
    "                    output_real = self.netD(real_images)\n",
    "\n",
    "                    # Produce fake images\n",
    "                    noise = torch.randn((batch_size, self.netG.module.nz), device=self.device)\n",
    "                    fake_images = self.netG(noise).detach()\n",
    "\n",
    "                    # Produce logits for fake images\n",
    "                    output_fake = self.netD(fake_images)\n",
    "\n",
    "                    # Compute loss for D\n",
    "                    errD = self.netD.module.compute_gan_loss(output_real=output_real,\n",
    "                                                             output_fake=output_fake)\n",
    "\n",
    "                    # Backprop and update gradients\n",
    "                    errD.backward()\n",
    "                    self.optD.step()\n",
    "\n",
    "                    # Compute probabilities\n",
    "                    D_x, D_Gz = self.netD.module.compute_probs(output_real=output_real,\n",
    "                                                               output_fake=output_fake)\n",
    "\n",
    "                    # Log statistics for D once out of loop\n",
    "                    log_data.add_metric('errD', errD.item(), group='loss')\n",
    "                    log_data.add_metric('D(x)', D_x, group='prob')\n",
    "                    log_data.add_metric('D(G(z))', D_Gz, group='prob')\n",
    "\n",
    "                    # -----------------------\n",
    "                    #   Update G Network\n",
    "                    # -----------------------\n",
    "                    # Update G, but only once.\n",
    "                    if i == (self.n_dis - 1):\n",
    "\n",
    "                        self.netG.module.zero_grad()\n",
    "\n",
    "                        # Get only batch size from real batch\n",
    "                        batch_size = real_batch[0].shape[0]\n",
    "\n",
    "                        # Produce fake images\n",
    "                        noise = torch.randn((batch_size, self.netG.module.nz), device=self.device)\n",
    "                        fake_images = self.netG(noise)\n",
    "\n",
    "                        # Compute output logit of D thinking image real\n",
    "                        output = self.netD(fake_images)\n",
    "\n",
    "                        # Compute loss\n",
    "                        errG = self.netG.module.compute_gan_loss(output=output)\n",
    "\n",
    "                        # Backprop and update gradients\n",
    "                        errG.backward()\n",
    "                        self.optG.step()\n",
    "\n",
    "                        # Log statistics\n",
    "                        log_data.add_metric('errG', errG, group='loss')\n",
    "\n",
    "                # --------------------------------\n",
    "                #   Update Training Variables\n",
    "                # -------------------------------\n",
    "                log_data = self.scheduler.step(log_data=log_data,\n",
    "                                               global_step=global_step)\n",
    "\n",
    "                # -------------------------\n",
    "                #   Logging and Metrics\n",
    "                # -------------------------\n",
    "                if global_step % self.log_steps == 0:\n",
    "                    self.logger.write_summaries(log_data=log_data,\n",
    "                                                global_step=global_step)\n",
    "\n",
    "                if global_step % self.print_steps == 0:\n",
    "                    curr_time = time.time()\n",
    "                    self.logger.print_log(global_step=global_step,\n",
    "                                          log_data=log_data,\n",
    "                                          time_taken=(curr_time - start_time) /\n",
    "                                          self.print_steps)\n",
    "                    start_time = curr_time\n",
    "\n",
    "                if global_step % self.vis_steps == 0:\n",
    "                    self.logger.vis_images(netG=self.netG.module if isinstance(self.netG, nn.DataParallel) else self.netG,\n",
    "                                           global_step=global_step)\n",
    "\n",
    "                if global_step % self.save_steps == 0:\n",
    "                    print(\"INFO: Saving checkpoints...\")\n",
    "                    self._save_model_checkpoints(global_step)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "            print(\"INFO: Saving final checkpoints...\")\n",
    "            self._save_model_checkpoints(global_step)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"INFO: Saving checkpoints from keyboard interrupt...\")\n",
    "            self._save_model_checkpoints(global_step)\n",
    "\n",
    "        finally:\n",
    "            self.logger.close_writers()\n",
    "\n",
    "        print(\"INFO: Training Ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 101,
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "class MCRTrainer(MUltiGPUTrainer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 netD,\n",
    "                 netG,\n",
    "                 optD,\n",
    "                 optG,\n",
    "                 dataloader,\n",
    "                 num_steps,\n",
    "                 log_dir='./log',\n",
    "                 n_dis=1,\n",
    "                 lr_decay=None,\n",
    "                 device=None,\n",
    "                 netG_ckpt_file=None,\n",
    "                 netD_ckpt_file=None,\n",
    "                 print_steps=1,\n",
    "                 vis_steps=500,\n",
    "                 log_steps=50,\n",
    "                 save_steps=5000,\n",
    "                 flush_secs=30,\n",
    "                 num_class=1000,\n",
    "                 mode=0):\n",
    "\n",
    "        super(MCRTrainer, self).__init__(netD, netG, optD, optG, dataloader, num_steps, log_dir, n_dis, lr_decay,\n",
    "                                         device, netG_ckpt_file, netD_ckpt_file, print_steps, vis_steps, log_steps,\n",
    "                                         save_steps, flush_secs)\n",
    "        cfg = _C\n",
    "        self.mcr_gan_loss = MCRGANloss(gam1=cfg.LOSS.GAM1, gam2=cfg.LOSS.GAM2, gam3=cfg.LOSS.GAM3, eps=cfg.LOSS.EPS, numclasses=num_class, mode=mode, rho=cfg.LOSS.RHO)\n",
    "\n",
    "    def show(self, imgs, epoch, name):\n",
    "        if not isinstance(imgs, list):\n",
    "            imgs = [imgs]\n",
    "        fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "        for i, img in enumerate(imgs):\n",
    "            img = img.detach()\n",
    "            img = FF.to_pil_image(img)\n",
    "            axs[0, i].imshow(np.asarray(img))\n",
    "            axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        if not os.path.exists(f\"{self.log_dir}/images\"):\n",
    "            os.makedirs(f\"{self.log_dir}/images\")\n",
    "        plt.savefig(f\"{self.log_dir}/images/{epoch:07d}_{name}.png\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "                Runs the training pipeline with all given parameters in Trainer.\n",
    "                \"\"\"\n",
    "        # Restore models\n",
    "        cfg = _C\n",
    "        self.parallel = isinstance(self.netG, nn.DataParallel)\n",
    "\n",
    "        try:\n",
    "            global_step = self._restore_models_and_step()\n",
    "            print(\"INFO: Starting training from global step {}...\".format(\n",
    "                global_step))\n",
    "\n",
    "            iter_dataloader = infiniteloop(self.dataloader)\n",
    "            nz = self.netD.module.nz\n",
    "\n",
    "            start_time = time.time()\n",
    "            while global_step < self.num_steps:\n",
    "                log_data = metric_log.MetricLog()  # log data for tensorboard\n",
    "\n",
    "                data_time = time.time()\n",
    "                data, label = next(iter_dataloader)\n",
    "                data_time = time.time() - data_time\n",
    "\n",
    "                # Format batch and label\n",
    "                real_cpu = data.to(self.device)\n",
    "                if cfg.DATA.DATASET == 'cifar10_data_aug_loop':\n",
    "                    real_cpu = torch.split(real_cpu, [3, 3], dim=1)\n",
    "                    # print(\"real_cup:\", real_cpu[0].size())\n",
    "                    real_cpu = torch.cat(real_cpu, 0)\n",
    "                    # print(\"real_cup:\", real_cpu.size())\n",
    "\n",
    "                real_label = label.clone().detach()\n",
    "\n",
    "                for i in range(self.n_dis):\n",
    "\n",
    "                    # Update Discriminator\n",
    "                    self.netD.zero_grad()\n",
    "                    self.optD.zero_grad()\n",
    "\n",
    "                    # Forward pass real batch through D(X->Z)\n",
    "                    Z = self.netD(real_cpu)\n",
    "\n",
    "                    # Generate batch of latent vectors (Z->X')\n",
    "                    X_bar = self.netG(torch.reshape(Z, (len(Z), nz)))\n",
    "\n",
    "                    # Forward pass fake batch through D(X'->Z')\n",
    "                    Z_bar = self.netD(X_bar.detach())\n",
    "\n",
    "                    # Optimize Delta R(Z)+deltaR(Z')+sum(delta(R(Z,Z'))) by alternating G/D\n",
    "                    errD, errD_EC = self.mcr_gan_loss(Z, Z_bar, real_label, i, self.n_dis)\n",
    "\n",
    "                    errD.backward()\n",
    "                    self.optD.step()\n",
    "\n",
    "                # Update Discriminator\n",
    "                self.netG.zero_grad()\n",
    "                self.optG.zero_grad()\n",
    "\n",
    "                # Repeat (X->Z->X'->Z')\n",
    "                Z = self.netD(real_cpu)\n",
    "                X_bar = self.netG(torch.reshape(Z, (len(Z), nz)))\n",
    "                Z_bar = self.netD(X_bar)\n",
    "\n",
    "                errG, errG_EC = self.mcr_gan_loss(Z, Z_bar, real_label, self.n_dis - 1, self.n_dis)\n",
    "\n",
    "                errG = (-1) * errG\n",
    "                errG.backward()\n",
    "                self.optG.step()\n",
    "\n",
    "                log_data.add_metric('errD', -errD.item(), group='discriminator loss')\n",
    "                log_data.add_metric('errG', -errG.item(), group='generator loss')\n",
    "\n",
    "                if self.mcr_gan_loss.train_mode == 0:\n",
    "                    log_data.add_metric('errD_E', -errD_EC[0].item(), group='discriminator loss')\n",
    "                    log_data.add_metric('errD_C', -errD_EC[1].item(), group='discriminator loss')\n",
    "\n",
    "                    log_data.add_metric('errG_E', -errG_EC[0].item(), group='generator loss')\n",
    "                    log_data.add_metric('errG_C', -errG_EC[1].item(), group='generator loss')\n",
    "\n",
    "                elif self.mcr_gan_loss.train_mode in [1, 2]:\n",
    "                    log_data.add_metric('errD_item1', -errD_EC[0].item(), group='discriminator loss')\n",
    "                    log_data.add_metric('errD_item2', -errD_EC[1].item(), group='discriminator loss')\n",
    "                    log_data.add_metric('errD_item3', -errD_EC[2].item(), group='discriminator loss')\n",
    "\n",
    "                    log_data.add_metric('errG_item1', -errG_EC[0].item(), group='generator loss')\n",
    "                    log_data.add_metric('errG_item2', -errG_EC[1].item(), group='generator loss')\n",
    "                    log_data.add_metric('errG_item3', -errG_EC[2].item(), group='generator loss')\n",
    "                elif self.mcr_gan_loss.train_mode in [10, ]:\n",
    "                    nlist = [\n",
    "                        'raw_deltaRz', 'raw_deltaRzbar', 'raw_sum_deltaRzzbar',\n",
    "                        'aug_deltaRz', 'aug_deltaRzbar', 'aug_sum_deltaRzzbar',\n",
    "                        'sum_deltaR_raw_z_aug_zbar', 'sum_deltaR_raw_z_aug_z'\n",
    "                    ]\n",
    "                    for i, name in enumerate(nlist):\n",
    "                        log_data.add_metric('errD'+name, -errD_EC[i].item(), group='discriminator loss')\n",
    "                        log_data.add_metric('errG'+name, -errG_EC[i].item(), group='generator loss')\n",
    "\n",
    "                else:\n",
    "                    raise ValueError()\n",
    "\n",
    "                log_data = self.scheduler.step(log_data=log_data,\n",
    "                                               global_step=global_step)\n",
    "\n",
    "                # -------------------------\n",
    "                #   Logging and Metrics\n",
    "                # -------------------------\n",
    "                if global_step % self.log_steps == 0:\n",
    "                    self.logger.write_summaries(log_data=log_data,\n",
    "                                                global_step=global_step)\n",
    "\n",
    "                if global_step % self.print_steps == 0:\n",
    "                    curr_time = time.time()\n",
    "                    self.logger.print_log(global_step=global_step,\n",
    "                                          log_data=log_data,\n",
    "                                          time_taken=(curr_time - start_time) /\n",
    "                                                     self.print_steps)\n",
    "                    print(\"data load time: \", data_time)\n",
    "                    print(f\"[{global_step % len(self.dataloader)}/{len(self.dataloader)}]\")\n",
    "                    print(self.log_dir)\n",
    "                    start_time = curr_time\n",
    "\n",
    "                if global_step % self.vis_steps == 0:\n",
    "                    # viz random noise\n",
    "                    # self.logger.vis_images(netG=self.netG.module if isinstance(self.netG, nn.DataParallel) else self.netG,\n",
    "                    #                        global_step=global_step)\n",
    "                    # viz auto-encoding\n",
    "                    with torch.no_grad():\n",
    "                        real = self.netG(torch.reshape(Z[:64], (64, nz))).detach().cpu()\n",
    "                        self.show(vutils.make_grid(real, padding=2, normalize=True), global_step, \"transcript\")\n",
    "                        self.show(vutils.make_grid(real_cpu[:64], padding=2, normalize=True), global_step, \"input\")\n",
    "\n",
    "                if global_step % self.save_steps == 0:\n",
    "                    print(\"INFO: Saving checkpoints...\")\n",
    "                    self._save_model_checkpoints(global_step)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "            print(\"INFO: Saving final checkpoints...\")\n",
    "            self._save_model_checkpoints(global_step)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"INFO: Saving checkpoints from keyboard interrupt...\")\n",
    "            self._save_model_checkpoints(global_step)\n",
    "\n",
    "        finally:\n",
    "            self.logger.close_writers()\n",
    "\n",
    "        print(\"INFO: Training Ended.\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 102,
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "def get_dataloader(data_name, root, batch_size, num_workers):\n",
    "\n",
    "    if data_name in [\"lsun_bedroom_128\", \"cifar10\", \"stl10_48\"]:\n",
    "        dataset = load_dataset(root=root, name=data_name)\n",
    "\n",
    "    elif data_name == 'celeba':\n",
    "        dataset = celeba_dataset(root=root, size=128)\n",
    "\n",
    "    elif data_name == 'mnist':\n",
    "\n",
    "        transform = transforms.Compose(\n",
    "                    [transforms.Resize(32),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(0.5, 0.5)])\n",
    "        dataset = datasets.MNIST(root=root, train=True, download=True, transform=transform)\n",
    "\n",
    "    elif data_name == 'TMNIST':\n",
    "        transform = transforms.Compose(\n",
    "                    [transforms.Resize(32),\n",
    "                    transforms.ToTensor(),\n",
    "                    MyAffineTransform(choices=[[0, 1], [0, 1.5], [0, 0.5], [-45, 1], [45, 1]]),\n",
    "                    transforms.Normalize(0.5, 0.5)])\n",
    "        dataset = datasets.MNIST(root=root, train=True,\n",
    "                                    download=True, transform=transform)\n",
    "\n",
    "    elif data_name == 'imagenet_128':\n",
    "        dataset = datasets.ImageFolder(root,\n",
    "                                       transform=transforms.Compose([\n",
    "                                         transforms.CenterCrop(224),\n",
    "                                         transforms.Resize(size=(128, 128)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "                                         transforms.Lambda(lambda x: x + torch.rand_like(x) / 128)\n",
    "                                       ]))\n",
    "\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 103,
=======
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "_C = CN()\n",
    "_C.LOG_DIR = 'logs/mnist_LDR_multi'\n",
    "# _C.GPUS = (0,)\n",
    "\n",
    "_C.CUDNN = CN()\n",
    "_C.CUDNN.BENCHMARK = True\n",
    "_C.CUDNN.DETERMINISTIC = False\n",
    "_C.CUDNN.ENABLED = True\n",
    "_C.CUDNN.WORKERS = 4\n",
    "\n",
    "# dataset\n",
    "_C.DATA = CN()\n",
    "_C.DATA.ROOT = './data/'\n",
    "_C.DATA.DATASET = 'mnist'\n",
    "_C.DATA.IMAGE_SIZE = [32, 32]\n",
    "_C.DATA.NC = 3\n",
    "\n",
    "# common params for NETWORK\n",
    "_C.MODEL = CN()\n",
    "_C.MODEL.NUM_CLASSES = 10\n",
    "_C.MODEL.CIFAR_BACKBONE = ''\n",
    "_C.MODEL.INIT = ''\n",
    "_C.MODEL.L_RELU_P = 0.2\n",
    "_C.MODEL.IMAGENET_WIDTH = 1024\n",
    "_C.MODEL.NZ = 100  # Size of z latent vector (i.e. size of generator input)\n",
    "_C.MODEL.NGF = 64  # Size of feature maps in generator\n",
    "_C.MODEL.NDF = 64  # Size of feature maps in discriminator\n",
    "\n",
    "# loss\n",
    "_C.LOSS = CN()\n",
    "_C.LOSS.MODE = 0  # 0 for LDR-binary, 1 for LDR multi\n",
    "_C.LOSS.GAM1 = 1.\n",
    "_C.LOSS.GAM2 = 1.\n",
    "_C.LOSS.GAM3 = 1.\n",
    "_C.LOSS.EPS = 0.5\n",
    "_C.LOSS.RHO = (1.0, 1.0)\n",
    "\n",
    "# training\n",
    "_C.TRAIN = CN()\n",
    "_C.TRAIN.BATCH_SIZE = 2048\n",
    "_C.TRAIN.LR_D = 0.00015\n",
    "_C.TRAIN.LR_G = 0.00015\n",
    "_C.TRAIN.BETA1 = 0.5  # Beta1 hyperparam for Adam optimizers\n",
    "_C.TRAIN.BETA2 = 0.999  # Beta2 hyperparam for Adam optimizers\n",
    "_C.TRAIN.ITERATION = 4500  # number of total iterations\n",
    "_C.TRAIN.INNER_LOOP = 1\n",
    "_C.TRAIN.LR_DECAY = 'linear'\n",
    "_C.TRAIN.SHOW_STEPS = 100\n",
    "_C.TRAIN.SAVE_STEPS = 5000\n",
    "\n",
    "# evaluation\n",
    "_C.EVAL = CN()\n",
    "_C.EVAL.DATA_SAMPLE = 50000\n",
    "_C.EVAL.NETD_CKPT = ''\n",
    "_C.EVAL.NETG_CKPT = ''\n",
    "\n",
    "\n",
    "def update_config(cfg, args):\n",
    "    cfg.defrost()\n",
    "    if args.cfg:\n",
    "        cfg.merge_from_file(args.cfg)\n",
    "\n",
    "    # if args.testModel:\n",
    "    #     cfg.TEST.MODEL_FILE = args.testModel\n",
    "\n",
    "    cfg.merge_from_list(args.opts)\n",
    "\n",
    "    cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 104,
=======
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "def get_models(data_name, device):\n",
    "\n",
    "    if data_name == \"cifar10\":\n",
    "        netG, netD = get_cifar_model()\n",
    "        netG = nn.DataParallel(netG.to(device))\n",
    "        netD = nn.DataParallel(netD.to(device))\n",
    "    elif data_name in ['mnist', 'TMNIST']:\n",
    "        netG = GeneratorMNIST().to(device)\n",
    "        netG.apply(weights_init_mnist_model)\n",
    "        netG = nn.DataParallel(netG)\n",
    "        netD = DiscriminatorMNIST().to(device)\n",
    "        netD.apply(weights_init_mnist_model)\n",
    "        netD = nn.DataParallel(netD)\n",
    "    elif data_name == 'stl10_48':\n",
    "        netG = sngan.SNGANGenerator48().to(device)\n",
    "        netG = nn.DataParallel(netG)\n",
    "\n",
    "        netD = customSNGANDiscriminator48().to(device)\n",
    "        netD = nn.DataParallel(netD)\n",
    "    elif data_name in [\"celeba\", \"lsun_bedroom_128\", \"imagenet_128\"]:\n",
    "        netG = sngan.SNGANGenerator128(ngf=cfg.MODEL.IMAGENET_WIDTH).to(device)\n",
    "        netG = nn.DataParallel(netG)\n",
    "\n",
    "        netD = customSNGANDiscriminator128(ndf=cfg.MODEL.IMAGENET_WIDTH).to(device)\n",
    "        netD = nn.DataParallel(netD)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    return netD, netG"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 105,
=======
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "source": [
    "def run_ldr():\n",
    "    \"\"\"the default setting is running the binary LDR on different data-sets.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "    config = _C\n",
    "    dataloader, dataset = get_dataloader(\n",
    "        data_name=config.DATA.DATASET,\n",
    "        root=config.DATA.ROOT,\n",
    "        batch_size=config.TRAIN.BATCH_SIZE,\n",
    "        num_workers=config.CUDNN.WORKERS\n",
    "    )\n",
    "\n",
    "    # Define models and optimizers\n",
    "    netD, netG = get_models(config.DATA.DATASET, device)\n",
    "\n",
    "    optD = optim.Adam(netD.parameters(), config.TRAIN.LR_D, betas=(config.TRAIN.BETA1, config.TRAIN.BETA2))\n",
    "    optG = optim.Adam(netG.parameters(), config.TRAIN.LR_G, betas=(config.TRAIN.BETA1, config.TRAIN.BETA2))\n",
    "\n",
    "    # Start training\n",
    "    trainer = MCRTrainer(\n",
    "        netD=netD,\n",
    "        netG=netG,\n",
    "        optD=optD,\n",
    "        optG=optG,\n",
    "        n_dis=config.TRAIN.INNER_LOOP,\n",
    "        num_steps=config.TRAIN.ITERATION,\n",
    "        lr_decay=config.TRAIN.LR_DECAY,\n",
    "        print_steps=config.TRAIN.SHOW_STEPS,\n",
    "        vis_steps=config.TRAIN.SHOW_STEPS,\n",
    "        log_steps=config.TRAIN.SHOW_STEPS,\n",
    "        save_steps=config.TRAIN.SAVE_STEPS,\n",
    "        dataloader=dataloader,\n",
    "        log_dir=config.LOG_DIR,\n",
    "        device=device,\n",
    "        num_class=config.MODEL.NUM_CLASSES,\n",
    "        mode=config.LOSS.MODE,\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 107,
   "source": [
    "run_ldr()"
   ],
=======
   "execution_count": 47,
   "metadata": {},
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Restoring checkpoint for D...\n",
      "INFO: Restoring checkpoint for G...\n",
      "INFO: Starting training from global step 1...\n",
      "INFO: Saving checkpoints from keyboard interrupt...\n",
      "INFO: Training Ended.\n"
     ]
    },
    {
<<<<<<< HEAD
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/vishalraman/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
=======
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-681e137cb17a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORKERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_ldr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-f7ad3da23a94>\u001b[0m in \u001b[0;36mrun_ldr\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORKERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-baa66686088b>\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(data_name, root, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     transforms.Normalize(0.5, 0.5)])\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'TMNIST'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
>>>>>>> 2cb70957fd33bfaf2ab0ddafa30f2b21d040b4d7
     ]
    }
   ],
   "source": [
    "print(_C.CUDNN.WORKERS)\n",
    "run_ldr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5bbe500713493ff1dfca1f22912d533f01e43ffe6d64033a8e8ceec5edef9de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
