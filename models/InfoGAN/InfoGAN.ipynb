{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#from https://github.com/Yangyangii/GAN-Tutorial/blob/master/MNIST/infoGAN.ipynb\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os, sys\n",
    "from matplotlib.pyplot import imshow, imsave\n",
    "%matplotlib inline\n",
    "\n",
    "MODEL_NAME = 'infoGAN'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def to_onehot(x, num_classes=10):\n",
    "    assert isinstance(x, int) or isinstance(x, (torch.LongTensor, torch.cuda.LongTensor))\n",
    "    if isinstance(x, int):\n",
    "        c = torch.zeros(1, num_classes).long()\n",
    "        c[0][x] = 1\n",
    "    else:\n",
    "        x = x.cpu()\n",
    "        c = torch.LongTensor(x.size(0), num_classes)\n",
    "        c.zero_()\n",
    "        c.scatter_(1, x, 1) # dim, index, src value\n",
    "    return c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def sample_noise(batch_size, n_noise, n_c_discrete, n_c_continuous, label=None, supervised=False):\n",
    "    z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "    if supervised:\n",
    "        c_discrete = to_onehot(label).to(DEVICE) # (B,10)\n",
    "    else:\n",
    "        c_discrete = to_onehot(torch.LongTensor(batch_size, 1).random_(0, n_c_discrete)).to(DEVICE) # (B,10)\n",
    "    c_continuous = torch.zeros(batch_size, n_c_continuous).uniform_(-1, 1).to(DEVICE) # (B,2)\n",
    "    c = torch.cat((c_discrete.float(), c_continuous), 1)\n",
    "    return z, c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_sample_image():\n",
    "    \"\"\"\n",
    "        save sample 100 images\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    # continuous code\n",
    "    for cc_type in range(2):\n",
    "        for num in range(10):\n",
    "            fix_z = torch.randn(1, n_noise)\n",
    "            z = fix_z.to(DEVICE)\n",
    "            cc = -1\n",
    "            for i in range(10):\n",
    "                cc += 0.2\n",
    "                c_discrete = to_onehot(num).to(DEVICE) # (B,10)\n",
    "                c_continuous = torch.zeros(1, n_c_continuous).to(DEVICE)\n",
    "                c_continuous.data[:,cc_type].add_(cc)\n",
    "                c = torch.cat((c_discrete.float(), c_continuous), 1)\n",
    "                y_hat = G(z, c)\n",
    "                line_img = torch.cat((line_img, y_hat.view(28, 28)), dim=1) if i > 0 else y_hat.view(28, 28)\n",
    "            all_img = torch.cat((all_img, line_img), dim=0) if num > 0 else line_img\n",
    "        img = all_img.cpu().data.numpy()\n",
    "        images.append(img)\n",
    "    # discrete code\n",
    "    for num in range(10):\n",
    "        c_discrete = to_onehot(num).to(DEVICE) # (B,10)\n",
    "        for i in range(10):\n",
    "            z = torch.randn(1, n_noise).to(DEVICE)\n",
    "            c_continuous = torch.zeros(1, n_c_continuous).to(DEVICE)\n",
    "            c = torch.cat((c_discrete.float(), c_continuous), 1)\n",
    "            y_hat = G(z, c)\n",
    "            line_img = torch.cat((line_img, y_hat.view(28, 28)), dim=1) if i > 0 else y_hat.view(28, 28)\n",
    "        all_img = torch.cat((all_img, line_img), dim=0) if num > 0 else line_img\n",
    "    img = all_img.cpu().data.numpy()\n",
    "    images.append(img)\n",
    "    return images[0], images[1], images[2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def log_gaussian(c, mu, var):\n",
    "    \"\"\"\n",
    "        criterion for Q(condition classifier)\n",
    "    \"\"\"\n",
    "    return -((c - mu)**2)/(2*var+1e-8) - 0.5*torch.log(2*np.pi*var+1e-8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Discriminator for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(# 28 -> 14\n",
    "            nn.Conv2d(in_channel, 64, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(# 14 -> 7\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(#\n",
    "            nn.Linear(128*7*7, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 12),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_ = self.layer1(x)\n",
    "        y_ = self.layer2(y_)\n",
    "        y_ = y_.view(y_.size(0), -1)\n",
    "        y_ = self.layer3(y_)\n",
    "        d = self.fc(y_) # Real / Fake        \n",
    "        return d, y_ # return with top layer features for Q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class Qrator(nn.Module):\n",
    "    \"\"\"\n",
    "        Regularization Network for increasing Mutual Information\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Qrator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 14),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Seperate code\n",
    "        c = self.fc(x)\n",
    "        c_discrete = torch.softmax(c[:, :10], dim=-1) # Digit Label {0~9}\n",
    "        c_mu = c[:, 10:12] # mu & var of Rotation & Thickness\n",
    "        c_var = c[:, 12:14].exp() # mu & var of Rotation & Thickness\n",
    "        return c_discrete, c_mu, c_var"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Generator for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=62, code_size=12, num_classes=784):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(12, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(1024, 7*7*128),\n",
    "            nn.BatchNorm1d(7*7*128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(# input: 7 by 7, output: 14 by 14\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(# input: 14 by 14, output: 28 by 28\n",
    "            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, c):\n",
    "        z = z.view(z.size(0), -1)\n",
    "        c = c.view(c.size(0), -1)\n",
    "        noise = torch.cat((z, c), 1)\n",
    "#         print(noise.size())\n",
    "        x_ = self.layer1(noise)\n",
    "        x_ = self.layer2(x_)\n",
    "        x_ = x_.view(x_.size(0), 128, 7, 7)\n",
    "        x_ = self.layer3(x_)\n",
    "        x_ = self.layer4(x_)\n",
    "        return x_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "D = Discriminator().to(DEVICE)\n",
    "G = Generator().to(DEVICE)\n",
    "Q = Qrator().to(DEVICE)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5],\n",
    "                                std=[0.5])])\n",
    "mnist = datasets.MNIST(root='../data/', train=True, transform=transform, download=True)\n",
    "batch_size = 128\n",
    "data_loader = DataLoader(dataset=mnist, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "bce_loss = nn.BCELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.99))\n",
    "G_opt = torch.optim.Adam([{'params':G.parameters()}, {'params':Q.parameters()}], lr=1e-3, betas=(0.5, 0.99))\n",
    "max_epoch = 50 # need more than 200 epochs for training generator\n",
    "step = 0\n",
    "n_critic = 1 # for training more k steps about Discriminator\n",
    "n_noise = 62\n",
    "n_c_discrete, n_c_continuous = 10, 2\n",
    "\n",
    "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator Label to real\n",
    "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label to fake\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def train():\n",
    "    step = 0\n",
    "    for epoch in range(max_epoch+1):\n",
    "        for idx, (images, labels) in enumerate(data_loader):\n",
    "            step += 1\n",
    "            labels = labels.view(batch_size, 1)\n",
    "            # Training Discriminator\n",
    "            x = images.to(DEVICE)\n",
    "            x_outputs, _, = D(x)\n",
    "            D_x_loss = bce_loss(x_outputs, D_labels)\n",
    "\n",
    "            z, c = sample_noise(batch_size, n_noise, n_c_discrete, n_c_continuous, label=labels, supervised=True)\n",
    "            z_outputs, _, = D(G(z, c))\n",
    "            D_z_loss = bce_loss(z_outputs, D_fakes)\n",
    "            D_loss = D_x_loss + D_z_loss\n",
    "            \n",
    "            D_opt.zero_grad()\n",
    "            D_loss.backward()\n",
    "            D_opt.step()\n",
    "\n",
    "            # Training Generator\n",
    "            z, c = sample_noise(batch_size, n_noise, n_c_discrete, n_c_continuous, label=labels, supervised=True)\n",
    "            c_discrete_label = torch.max(c[:, :-2], 1)[1].view(-1, 1)\n",
    "\n",
    "            z_outputs, features = D(G(z, c)) # (B,1), (B,10), (B,4)\n",
    "            c_discrete_out, cc_mu, cc_var = Q(features)\n",
    "\n",
    "            G_loss = bce_loss(z_outputs, D_labels)\n",
    "            Q_loss_discrete = ce_loss(c_discrete_out, c_discrete_label.view(-1))\n",
    "            Q_loss_continuous = -torch.mean(torch.sum(log_gaussian(c[:, -2:], cc_mu, cc_var), 1)) # N(x | mu,var) -> (B, 2) -> (,1)\n",
    "            mutual_info_loss = Q_loss_discrete + Q_loss_continuous*0.1\n",
    "\n",
    "            GnQ_loss = G_loss + mutual_info_loss\n",
    "\n",
    "            G_opt.zero_grad()\n",
    "            GnQ_loss.backward()\n",
    "            G_opt.step()\n",
    "            if step % 10 == 0:\n",
    "                print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {}, GnQ Loss: {}, Time: {}'\\\n",
    "                    .format(epoch, max_epoch, step, D_loss.item(), G_loss.item(), GnQ_loss.item(), str(datetime.datetime.today())[:-7]))\n",
    "                \n",
    "            if step % 1000 == 0:\n",
    "                G.eval()\n",
    "                img1, img2, img3 = get_sample_image()\n",
    "                imsave('samples/{}_step{}_type1.jpg'.format(MODEL_NAME, str(step).zfill(3)), img1, cmap='gray')\n",
    "                imsave('samples/{}_step{}_type2.jpg'.format(MODEL_NAME, str(step).zfill(3)), img2, cmap='gray')\n",
    "                imsave('samples/{}_step{}_type3.jpg'.format(MODEL_NAME, str(step).zfill(3)), img3, cmap='gray')\n",
    "                G.train()\n",
    "    torch.save(\"infogan_disc.dat\", D_opt)\n",
    "    torch.save(\"infogan_gen.dat\", G_opt)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128, 12])) is deprecated. Please ensure they have the same size.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      9\u001b[0m x_outputs, _, \u001b[38;5;241m=\u001b[39m D(x)\n\u001b[0;32m---> 10\u001b[0m D_x_loss \u001b[38;5;241m=\u001b[39m \u001b[43mbce_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m z, c \u001b[38;5;241m=\u001b[39m sample_noise(batch_size, n_noise, n_c_discrete, n_c_continuous, label\u001b[38;5;241m=\u001b[39mlabels, supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m z_outputs, _, \u001b[38;5;241m=\u001b[39m D(G(z, c))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/functional.py:3056\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3054\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3057\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3059\u001b[0m     )\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3062\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128, 12])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('py39': conda)"
  },
  "interpreter": {
   "hash": "2f0dd2f7e7420ada810fc58588b9276dc0a307eacdcd2b6dddd3c59ce843bef6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}